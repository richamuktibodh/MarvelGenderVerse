{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Iron Man 3.txt line by line\n",
    "with open('Iron Man 3.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    # print(lines)\n",
    "\n",
    "# print(lines[0])\n",
    "# print(lines[1])\n",
    "# print(lines[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through lines, if line contains a colon, split the line into speaker and dialogue\n",
    "speaker_dialogue_rows = []\n",
    "for line in lines:\n",
    "    if ':' in line:\n",
    "        # print(line)\n",
    "        speaker, dialogue = line.split(':', 1)\n",
    "        # print(speaker)\n",
    "        # print(dialogue.strip())\n",
    "        # add the speaker and dialogue to the dataframe\n",
    "        speaker_dialogue_rows.append({'Speaker': speaker, 'Dialogue': dialogue.strip()})\n",
    "\n",
    "speaker_dialogue_mapping = pd.DataFrame(speaker_dialogue_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tony Stark</td>\n",
       "      <td>We create our own demons. Who said that? What ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tony Stark</td>\n",
       "      <td>I'm gonna start again. Let's track this from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Hogan</td>\n",
       "      <td>(to Maya.) Half hour till the ball drops.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tony Stark</td>\n",
       "      <td>Hey, do you want...?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Party Guest</td>\n",
       "      <td>Tony Stark? Great speech, man!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Speaker                                           Dialogue\n",
       "0   Tony Stark  We create our own demons. Who said that? What ...\n",
       "1   Tony Stark  I'm gonna start again. Let's track this from t...\n",
       "2  Happy Hogan          (to Maya.) Half hour till the ball drops.\n",
       "3   Tony Stark                               Hey, do you want...?\n",
       "4  Party Guest                     Tony Stark? Great speech, man!"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_dialogue_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe with key as speaker and all the speakers dialogues as values(the values are joined by a space)\n",
    "speaker_dialogue_mapping_grouped = speaker_dialogue_mapping.groupby('Speaker')['Dialogue'].apply(lambda x: ' '.join(x)).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aldrich Killian', 'Announcer', 'Bill Maher', 'Both', 'Brandt', 'Bruce Banner', 'Cameraman', 'Chad Davis', 'Colonel James Rhodes', 'Computer Voice', 'Crew', 'EMCEE', 'Elk Ridge', 'Erin', 'Gary', 'Government Employee', 'Government Official #1', 'Government Official #2', 'Guard', 'Guard #1', 'Guard #2', 'Happy Hogan', \"Happy's Nurse\", 'Harley Keener', 'Heather', 'Ho Yinsen', 'Hospital News Reporter', 'JARVIS', 'Jarvis', 'Joan Rivers', 'Little Boy', 'Man', 'Mandarin Look-Out', 'Maya Hansen', 'Military Aide', 'Mrs. Davis', 'News Reporter #1', 'News Reporter #2', 'News Reporter #3', 'News Reporter #4', 'Nurse', 'Officer', 'Officer #2', 'Party Guest', 'Pepper Potts', \"Pepper's Assistant\", 'President Ellis', 'Pushy Tabloid Reporter', 'Rhodey', 'Rose Hill Christmas Tree Shopper', 'Rose Hills Sheriff', 'Savin', 'Taggert', 'The Mandarin', 'Thomas Richards', 'Tony Stark', 'Trevor Slattery', 'Vanessa', 'Vice President', 'Woman']\n"
     ]
    }
   ],
   "source": [
    "# print all the speakers and their dialogues\n",
    "# for index, row in speaker_dialogue_mapping_grouped.iterrows():\n",
    "    # print(row['Speaker'])\n",
    "    # print(row['Dialogue'])\n",
    "    # print('\\n')\n",
    "\n",
    "print(list(speaker_dialogue_mapping_grouped['Speaker']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aldrich Killian\n",
      "Mr. Stark! Oh, wow! Hey, Tony! Aldrich Killian. (Stuttering to Maya) I'm a big fan of your work! Well, of course. But, Miss Hansen, my organization has been tracking your research since year two of MIT. Oh, now, that is an appropriate question. The ground floor, actually. I've got a proposal I'm putting together with myself. It's a privately-funded think tank called, Advanced Idea Mechanics. [He holds out two business cards towards Tony and Maya] Advanced Idea Mechanics, or AIM for short. (Points to the logo on his shirt) Do you get it? Aw! Yeah? I'll see you up there. Pepper. You look great. You look really great. Nothing fancy, just five years in the hands of physical therapists. And please, call me Aldrich. After years dodging the President's ban on \"immoral biotech research\", my think tank now has a little something in the pipeline. It's an idea we like to call Extremis. I'm gonna turn your lights down. Regard the human brain. Uh...wait. Hold on, hold on. That's...that's the universe, my bad. But if I do that... That's the brain. Strangely mimetic though, wouldn't you say? Thanks, it's mine. This...you're inside my head. It's a... It's a live feed. Come on up, I'll prove it to you. Come on. Now, pinch my arm. I can take it. Pinch me. It's the primary somatosensory cortex. It's the brain's pain center. But this is what I wanted to show you. Now, Extremis harnesses our bioelectrical potential and it goes...here. This is essentially an empty slot, and what this tells us is that our mind, our entire DNA in fact, is destined to be upgraded. Imagine if you could hack into the hard drive of any living organism and recode its DNA. Mm. Tony. Tony. You know, I invited Tony to join AIM thirteen years ago, he turned me down. But something tells me now there is a new genius on the throne who doesn't have to answer to Tony anymore, and who has slightly less of an ego. Well, I can't say that I'm not disappointed. But then as my father used to say, 'Failure is the fog through which we glimpse triumph.' Mm Well, me neither. He was kind of an idiot, my old man. I'm sure I'll see you again, Pepper. Mmhmm. I see. I have to go. [Aldrich rises from his seat walks towards another room] The Master is about to record and he's a little... Well, you know how he gets. Keep your appointment tonight and call me when it's done. [Aldrich walks into a room which has been set up as a set with all of Mandarin's props in place and camera crew ready to record Mandarin's message] Alright everybody. No talking and no eye contact, unless you wanna get shot in the face. What would you regard as the defining moment of your life? Will you please state your name for the camera? Okay. So, the injections are administered periodically. Addiction will not be tolerated. And those who cannot regulate will be out from the programme. (More typing, another video.) Once misfits, cripples... You are the next iteration of human evolution. Everybody, before we start... I promise you, looking back at your life, there will be nothing as bitter as the memory of that glorious risk you prudently elected to forego. Today is your glory- Let's begin. (Brandt's missing arm grows back as she glows red. The other volunteers glow red & scream too.) We gotta get out of here! We gotta get out of here! Get her out! Get them out of here! (One of the volunteers exploads.) Hi, Pepper. (To Maya) So, you want to tell me why you were at Stark's mansion last night? Oh, I see. So, you were trying to save Stark when he threatened us? Pepper. Pepper. Pepper. You know what my old man used to say to me? One of his favourite of many sayings... \"The early bird gets the worm, but the second mouse gets the cheese.\" How can I be pissed at you, Tony? I'm here to thank you. You gave me the greatest gift that anybody's ever given me. Desperation. If you think back to Switzerland, you said you'd meet me on the rooftop, right? Well, for the first 20 minutes, I actually thought you'd show up. And the next hour... I considered taking that one-step shortcut to the lobby. If you know what I mean. But as I looked out over that city, nobody knew I was there, nobody could see me, no one was even looking. I had a thought that would guide me for years to come. Anonymity, Tony. Thanks to you, it's been my mantra ever since. Right? You simply rule from behind the scenes. Because the second you give evil a face, a bin Laden, a Gaddafi, a Mandarin, you hand the people a target. You have met him, I assume? I know he's a little over the top sometimes. It's not entirely my fault. He has a tenden... He's a stage actor. They say his Lear was the toast of Croydon, wherever that is. Anyway, the point is, ever since that big dude with the hammer fell out of the sky, subtlety has kind of had its day. Well, I wanted to repay you the selfsame gift that you so graciously imparted to me. [Killian rolls three balls forwards and they stop, projecting an image of Pepper. She has Extremis.] Desperation. Now, this is live. I'm not sure if you can tell, but at this moment the body is trying to decide whether to accept Extremis or just give up. And if it gives up, I have to say, the detonation is quite spectacular. But until that point, it's really just a lot of pain. We haven't even talked salary yet. What kind of perk package are you thinking of? Hold on, hold on. Maya... What are you doing? It's times like this my temper is tested somewhat. Maya, give me the injector. We're not doing this, okay? The good news is, a high-level position has just been vacated. No, I'm a visionary. But I do own a maniac. And he takes the stage tonight. [Killian leaves and talks to Savin] Once we get the Patriot installed, it will take me nine or 10 minutes for the takedown. Afternoon, gentlemen. [Whirring ceases] Hello, Colonel. Step aside. [Killian places his hand on the suit and it blows orange.] Oh! We'll get you out of there. Don't worry. Yes, I will. But you can fix it, right? I'm gonna take the Chinook to base camp. And I want Potts with me. You're not going deaf, are you? Are you coming out? It's a glorious day, Savin. This time tomorrow, I'll have the West's most powerful leader in one hand, and the world's most feared terrorist in the other. I'll own the war on terror. Create supply and demand. For you, for your brothers and sisters. [Pepper wakes up and gasps.] Hi. Having you here is not just to motivate Tony Stark. It's, um... Well, it's actually more embarrassing than that. You're here as my, um... [Chuckles.] Mmm. [Iron Patriot walks in and Pepper gasps.] Good evening, sir. [The armor opens and the President falls out.] Welcome aboard, Mr President. Ever hear of an elephant graveyard? Well, two years ago, the elephant in the room was this scow. And, of course, you'll remember that when she spilled a million gallons of crude off Pensacola, thanks to you, not one fat cat saw a day in court. Uh, nothing, sir. I just needed a reason to kill you that would play well on TV. You see, I've moved on. I found myself a new political patron, and this time tomorrow, he'll have your job. String him up. Okay. That's good. Now give me cameras A through E and we'll do a full tech rehearsal. (Technician typing) Is this guy bothering you? Don't get up. Ooh. Is it hot in there? [Sizzling.] Stuck? Do you feel a little stuck? Like a little turtle, cooking in his little turtle suit. She's watching. I think you should close your eyes. Close your eyes. Close your eyes. You don't want to see this. [Tony cuts off Killian's arm, it burns through the floor and Pepper falls down.] A shame. I would've caught her. [Tony and Killian run towards each other. Killian jumps up to attack, Tony slides under and armors up. Repulsors firing, flesh burning, both grunting.] Well, here we are on the roof. [Killian slices the suit in half.] You really didn't deserve her, Tony. It's a pity. I was so close to having her perfect. [Muffled screaming.] No! [He walks towards Tony, his flesh held together by flames.] No more false faces. You said you wanted the Mandarin. You're looking right at him. It was always me, Tony. Right from the start. *I AM THE MANDARIN!!!* [Someone hits Killian and he flies to the side. It's Pepper. Heavy breathing.]\n",
      "\n",
      "\n",
      "Announcer\n",
      "(On PA) Broadcast will commence shortly. Take final positions. [On PA] All personnel, we have hostiles on east unit 12. [On PA] I repeat, hostiles on east unit 12.\n",
      "\n",
      "\n",
      "Bill Maher\n",
      "And how is President Ellis responding? By taking the guy they call War Machine and giving him a paint job.\n",
      "\n",
      "\n",
      "Both\n",
      "...Genetic operating system... Human application.\n",
      "\n",
      "\n",
      "Brandt\n",
      "Thank you. [Tony notices the woman has burn marks on one side of her face] Nice watch. Oh, I don't doubt it. [there's a moment's pause] Well, have a good evening. [the woman turns and walks off] [Tony enters the bar and walks up to Mrs. Davis, who's sat at table drinking alone] Actually, I am. [Tony looks up and it's the woman he bumped into outside the bar] [suddenly Brandt grabs hold of Tony's arm and twists it, slamming his head onto the table, Tony manages to quickly grab hold of Chad's dog tags that were on the table] It's called an arrest. [she pushes Tony to the ground and steps towards the Sheriff] Sheriff, is it? Homeland Security. [she holds up her badge] We good here? Well, I think it's a little above your pay grade, Sheriff. Alright, you know what? I was hoping to do this the smart way, but uh...the fun way's always good. [Tony notices her hand turning red hot, burning the badge in her hand] That's all you got? Cheap trick and a cheesy one-liner? Ellen Brandt.\n",
      "\n",
      "\n",
      "Bruce Banner\n",
      "Sorry... I was, yeah. We were at, uh... [Stammering] I was... I... I drifted. Elevator in Switzerland. I'm sorry. I'm not that kind of doctor. I'm not a therapist. It's not my training. I don't have the... Temperament. Yes.\n",
      "\n",
      "\n",
      "Cameraman\n",
      "Because you erase my shows! We talked about this. Excuse me, sir. I don't know who... (Tony turns around.) Mom, I need to call you back. Something magical is happening. Tony Stark is in my van. Tony Stark is in my van! I knew you were still alive! Oh, wow. Can I just say, sir... I am your biggest fan. No, no, no. Just us. Gary\n",
      "\n",
      "\n",
      "Chad Davis\n",
      "Well, uh, I think that would be the day I decided not to let my injury beat me. (Tony types again and another video shows up.)\n",
      "\n",
      "\n",
      "Colonel James Rhodes\n",
      "It tested well with focus groups, alright? Listen, War Machine was a little too aggressive, alright? This sends a better message. It's classified information, Tony. Okay, there have been nine bombings. The public only knows about three. Here's the thing, nobody can ID a device. There's no bomb casings. When's the last time you got a good night's sleep? People are concerned about you, Tony. I'm concerned about you. No. No, look, I'm not trying to be a dic... ...tator. Fine with me. Listen, the Pentagon is scared. After what happened in New York... aliens, come on. They need to look strong. Stopping the Mandarin is priority, but it's not... No, it's not, quite frankly. It's American business. Are you okay? Take it easy. Tony... Wait a minute! Tony! Tony! Come on, man. This isn't a good look, open up.\n",
      "\n",
      "\n",
      "Computer Voice\n",
      "Stark Secure Server: Now transferring to all known receivers. Stark Secure Server. Retinal scan: Verified.\n",
      "\n",
      "\n",
      "Crew\n",
      "Do not erase a programme from my DVR unless you are 100% sure...\n",
      "\n",
      "\n",
      "EMCEE\n",
      "Very nice. Very nice. I have one question for you. What would you like for Christmas this year? One more time! Ms Elk Ridge, everybody! All right.\n",
      "\n",
      "\n",
      "Elk Ridge\n",
      "Well, David...\n",
      "\n",
      "\n",
      "Erin\n",
      "Do you mind signing my drawing? Erin. Are you okay, Mr. Stark?\n",
      "\n",
      "\n",
      "Gary\n",
      "Oh, wow. Okay. Oh, good. Can I just say? Yeah. I don't know if you can tell, but I have, like, patterned my whole look after you. My hair's a little... It's not right, 'cause there's no product in it. I don't want to make things awkward for you, but I do have to show you... Boom! Yeah. It's... I mean... I had them do it off a doll that I made, so it's not like it's off a picture. So it's a little bit... Got it. Yeah. And Gary needs Tony. Yeah.\n",
      "\n",
      "\n",
      "Government Employee\n",
      "Mr Vice President, I think you should see this.\n",
      "\n",
      "\n",
      "Government Official #1\n",
      "How did he hack my phone?\n",
      "\n",
      "\n",
      "Government Official #2\n",
      "We can't allow terrorists to dictate... I'd strongly advise against that.\n",
      "\n",
      "\n",
      "Guard\n",
      "[Beeping, Rhodey wakes up.] All personnel, Stark is loose and somewhere in the compound. Repeat, Stark is loose and somewhere in the compound.\n",
      "\n",
      "\n",
      "Guard #1\n",
      "832 miles. I'm good like that. Can you, uh, stop that?\n",
      "\n",
      "\n",
      "Guard #2\n",
      "I think I bought it. What are you gonna do to me? You're zip-tied to a bed. This. [Nothing happens.] That. [Nothing happens again] How did we get this shift? Wow. That was... I am just beyond terrified. Shut up.\n",
      "\n",
      "\n",
      "Happy Hogan\n",
      "(to Maya.) Half hour till the ball drops. I got you, pal. Edifying Yeah, we're full. What floor are you going to, pal? What? Down! Stay down! Stay down, boss. Stay down. What was that? It's not Y2K. Happy New Year. You good? I'll be right outside. Badge. Badge. Come on, badge. Badge, guys. I put a memo in the toilet, come on. [referring to Tony's robots as Hogan meets with Pepper] Tony has got them in his basement, they're wearing party hats. This is an asset that we can put to use. What I'm saying is that the human element of Human Resources is our biggest point of vulnerability. We should start phasing it out immediately. [to another employee as Hogan walks past them] Excuse me, Bambi. Security. Yes. Thank you. I do appreciate it You don't have to thank me. Thank you. That's not a compli...? It is a compliment! Clearly somebody's trying to hide something. Did you clear this four o'clock with me? How so? I don't like the sound of that. Uh...you were supposed to be issued a security badge. Yes? You sure? Okay. I'm gonna linger, right here. Okay. Hey, guy. Hello? What? You know, look, I got a real job. What do you want? I'm working, I got something going on here. Let me tell you something, you know what happened when I told people I was Iron Man's body guard? They would laugh in my face. I had to leave while I still had a shred of dignity. Now I got a real job, I'm watching Pepper. For real? Alright, so she's meeting up with this scientist. Rich guy, handsome. I couldn't make his face at first, right? You know I'm good with faces. Yeah. Well, so I run his credentials, I make him Aldrich Killian. We actually met the guy back in... where were we in '99? The science conference? Right, right, exactly. Of course you don't. He's not a blond with a big rack. At first it was fine, they were talking business, but now it's like getting weird. He's showing her a big brain. Big brain, and she likes it. Here, let me show you. Hold on. See? [he holds his tablet up and points it towards Pepper's glass office, where Aldrich and Pepper are standing close together on the coffee table watching the 3d image of the brain, but all Tony sees is Hogan pointing the tablet camera at himself] I'm not a tech genius like you. Just...just trust me, get down down here. I can't! I don't know how to flip the screen! Don't talk to me like that anymore. You're not my boss. Alright, I don't work for you. Now I don't trust this guy. He's got another guy with him, he's shifty. Seriously? You know what? You should take more of an interest in what's going on here. This woman... this woman's the best thing that ever happened to you, and you...you're just ignoring her. Yeah, there's a giant brain, there's a shifty character. I'm gonna follow this guy. I'm gonna run his plates and if it gets rough, so be it. Yeah, I miss you too. But the way it used to be. Now you're off with the 'superfriends', I don't know what's going on with you anymore. The world's getting weird... Why? Yeah, nice. The car is ready, if you're ready to go. I'm sorry, buddy. Yeah, a little movie called \"The Party's Over\", starring you and your junkie girlfriend, and here's the ticket. [Hogan shows him what he took from Taggert's suitcase] [Weak] No, look.\n",
      "\n",
      "\n",
      "Happy's Nurse\n",
      "Oh. Sure. [Tony rises from his seat]\n",
      "\n",
      "\n",
      "Harley Keener\n",
      "Freeze! Don't move. What's that thing on your chest? What does it power? [Tony stands and points the desk light at his suit sitting on the couch behind him] Oh my God! [Harley drops his toy gun and takes a step towards Tony] Oh my god! That... that's... Is that Iron Man? Technically, you're dead. [he give Tony a newspaper which has Tony's picture with the headline 'Mandarin Attack: Stark presumed dead'] [referring to the suit as he sits next to it on the couch checking it out] What happened to him? Like a mechanic? Oh. If I was building Iron Man and War Machine. That's way cooler! Anyways, I would have added in um... the retro... To make him stealth mode. Cool, right? [as he touches the suit, Harley accidentally snaps off one of its fingers] Oops! S...Sorry. Well, my mom already left for the diner and dad went to 7-Eleven to get scratchers. I... I guess he won, 'cause that was six years ago. What's in it for me? Who? How'd you know that? Deal. Harley. And you're... She's six! Anyway, it's limited edition. When can we talk about New York? What about The Avengers, can you talk about them? I guess this guy named Chad Davis, used to live roundabouts, won a bunch of medals in the army. One day, folks said he went crazy and made, you know, a bomb. Then he blew himself up right here. [Tony looks around at the remains] Yeah. Yeah. [Tony keeps looking around at the remains of the explosion site] Yeah, people said these shadows are like the mark of souls gone to Heaven. Except the bomb guy, he went to Hell on account of he didn't get a shadow. That's why there's only five. That's what everyone says. You know what this crater reminds me of? That giant wormhole, in um...in New York. Does it remind you? Are they coming back? The aliens? Does this subject make you...make you edgy? Are there bad guys in Rose Hills? Do you...do you need a plastic bag to breathe into? Do you have medication? Do you need to be on it? Do you have PTSD? Are you...are you going completely mental? I can stop, do you want me to stop? Do you want me to stop? What did I say? [Tony starts running off and Harley runs after him] Hey, wait up! Wait, wait. [Tony stops running and Harley catches up with him] What the hell was that? [Tony holds his face in his hands for a moment and then throws some snow at Harley] Where she always is. Let me go! [Savin mocks Harley] Mr. Stark, I am so sorry! You're welcome Me saving your life Unlike you? Admit it, you need me. We're connected. So now you're just going to leave me here, like my dad? [Childishly] I'm cold. It was worth a shot. Yeah, I'm still eating that candy. Do you want me to keep eating it? Two or three bowls. Sort of. Um, it does say Miami, Florida. Uh, it's not charging. [Tony pulls over to the side of the road, heavy breathing] Tony? [Tony gets out of the car] Are you having another attack? I didn't even mention New York. Okay, um, uh... Just breathe. Really, just breathe. You're a mechanic, right? You said so. Why don't you just build something?\n",
      "\n",
      "\n",
      "Heather\n",
      "[Screaming] Oh, God! No! No! What? Oh!\n",
      "\n",
      "\n",
      "Ho Yinsen\n",
      "Mr. Stark I would like to introduce you to our guest, Dr. Wu. Mr. Stark. Perhaps another time.\n",
      "\n",
      "\n",
      "Hospital News Reporter\n",
      "We're awaiting the arrival of Tony Stark. We're hoping he'll give us the reaction...his reaction to the latest attack. [Tony walks out and all the reporters swarm toward him] Mr. Stark! Mr. Stark! Our sources are telling us that this is another Mandarin attack. Anything else you can tell us? [Tony ignores the questions and walks toward his car]\n",
      "\n",
      "\n",
      "JARVIS\n",
      "Sir, please may I request just a few hours to calibrate... As you wish, sir. I've also prepared a safety briefing for you to entirely ignore. Sir, may I remind you that you've been awake for nearly seventy-two hours. As always, sir, a great pleasure watching you work. No sign of cardiac analomy or unusual brain activity. I've compiled a Mandarin database for you, sir. Drawn from S.H.I.E.L.D., F.B.I., and C.I.A. intercepts. Initiating virtual crime scene reconstruction. [Tony starts looking at all the data gathered] The heat from the blast was in excess of 3000 degrees Celsius. Any subjects within 12.5 yards were vaporized instantly. No, sir. Not according to public records, sir. The oracle cloud has completed analysis. Accessing satellites and plotting the last twelve months of thermogenic occurrence now. It predates any known Mandarin attack. The incident was the use of a bomb to assist a suicide. The heat signature is remarkably similar. Three thousand degrees Celsius. [Tony looks at all the information being presented on the Tennessee attack] Creating a flight plan for Tennessee. [as Tony is about to get ready to leave for Tennessee, he hears his door bell ring] There's only so much I can do, sir, when you give the world's press your home address. [we see Maya standing outside Tony's house, the glass doors are opened and she enters] Gluten-free waffles, sir. Sir, Miss Potts is clear of the structure. [Tony motions for his Iron Man suit to come off Pepper and onto him] Working on it, sir. This is a prototype. [Tony manages to fire a piano at a helicopter to destroy it] Sir, the suit is not combat-ready. [Tony manages to get away from the bullets being fired at him from one of the remaining helicopters and again uses his suit to bring down the helicopter] Sir, take a deep breath. [after a few moments Tony's suit gets its flight power activated by Jarvis] Flight power restored. [finally Tony manages to fly out of the ocean.] Sir. Sir! That's the emergency alert triggered by the power dropping below five percent. [Tony notices that he's flying through the snow at night, he falls and crashes into the ground in the middle of a forest, he takes off his face helmet as he lies on the ground catching his breath] We're five miles outside of Rose Hills, Tennessee. I prepared a flight plan. This was the location. I...I think I may be malfunctioning, sir. I actually think I need to sleep now, sir. [the suit loses power] It's totally fine, sir. I seem to do quite well for a stretch, and then at the end of the sentence I say the wrong cranberry. And, sir, you were right. Once I factored in available AIM downlink facilities I was able to pinpoint the Mandarin's broadcast signal. Actually, sir, it's in Miami. Actually, sir, it is charging, but the power source is questionable. It may not succeed in revitalising the Mark 42. Oh, hello, sir. Sir, I have an update from Malibu. The cranes have finally arrived, and the cellar doors are being cleared as we speak. The armour is now at 92%. Thirteen, sir. Four, sir. 18,000 feet. 10,000 feet. 6,000 feet. 1, 000 feet. 400 feet. 200 feet, sir. The House Party Protocol, sir? [Through armors] Yes, sir. Gentlemen. [Grunting, screaming.] Good evening, Colonel. Can I give you a lift? Sir, I've located Ms Potts. Mark 42, inbound. All wrapped up here, sir. Will there be anything else? The Clean Slate Protocol, sir?\n",
      "\n",
      "\n",
      "Jarvis\n",
      "My diagnosis is that you've experienced a severe anxiety attack.\n",
      "\n",
      "\n",
      "Joan Rivers\n",
      "Same suit, but painted red, white, and blue. Look at rhat. And they also renamed him Iron Patriot. You know, just in case the paint was too subtle.\n",
      "\n",
      "\n",
      "Little Boy\n",
      "[the little boy whispers to Tony] How did you get out of the wormhole? How did you get out of the wormhole?\n",
      "\n",
      "\n",
      "Man\n",
      "[On radio] This is support team Blue-Zero. Sending coordinates for a suspected Mandarin broadcast point of origin. [Scoffs] My fault again. Let me tell you something, sweetheart. I am not your personal air con... [Tony puts a gloved hand on his head, electrocuting him. He gets the gun and leaves the room. He makes his way to the bedroom and pulls the duvet off the bed, revealling two girls. Tony sushes them and the toilet flushes.] Honestly, I hate working here. They are so weird. [Tony leaves.] Everything okay, sir? Is anyone there? We made it! Over there!\n",
      "\n",
      "\n",
      "Mandarin Look-Out\n",
      "The Master is travelling. [the Mandarin arrives on set and takes a sit, he looks at Aldrich]\n",
      "\n",
      "\n",
      "Maya Hansen\n",
      "Unintelligible. Mmm-hmm. Where are we going? Okay, you can see my research, but that's...I'm not gonna show you my \"town.\" Bye. My work? Okay. [takes the cards] (to Aldrich as she walks out of the elevator) Thank you, I'll call you. Well, it was. If I'm right, we can access the area of the brain that governs repair... ...And chemically recode it. Exactly. Yes. Is that... Can you... Can you not touch my plant? It's not...she doesn't like it. She prefers... That's cute, but... [turns to Hogan] Because... and, no, seriously don't. [walks into the bedroom] For now, yeah. I'm calling it Extremis. Well, it's... Exactly! Exactly. Dendritic revitalization. Disease prevention... ...even limb regrowth. Wow. Hmm, that's better. Aw, you're seeing things. This is what I'm talking about, the glitch. The what? It's a glitch in my work. It's... No. Happy New Year. You don't remember. Why am I not surprised? Okay, look, I need to be alone with you. Someplace not here, it's urgent. He's thirteen. And no, I need your help. Because I read the papers, and, frankly, I don't think you'll last the week. No, I... No, not really. It...it was just one night. Yeah. I'm sure. Great idea. Let's go. Is...is that normal? Guys, can we um... [she points to the bomb heading straight for the house] Do we need to worry about that? [The house is suddenly hit, as everything explodes around them, Tony manages to get his Iron Man suit onto Pepper to protect her from the fall, Tony looks over to Maya, who's lying unconscious on the ground, as the ceiling is about to fall on Tony, Pepper saves him with the Iron Man suit] [A beat] I think that my boss is working for the Mandarin. So if you still want to talk about it, I suggest that we get ourselves someplace safe. That figures. What I actually am is a biological DNA coder running a team of forty out of a privately-funded think tank, but sure you can call me a botanist. Yeah, Aldrich Killian. [Pepper looks at her in shock] What happened? Fun fact. Before he built rockets for the Nazis, the idealistic Wernher von Braun dreamed of space travel. He stargazed. Do you know what he said when the first V-2 hit London? \"The rocket performed perfectly. It just landed on the wrong planet.\" See, we all begin wide-eyed. Pure science. And then the ego steps in, the obsession. And you look up, you're a long way from shore. Yeah, but Killian built that think tank on military contracts. Thank you, Pepper. I really appreciate that. (Someone knocks on the door & Pepper looks over. She gets up & opens the door. Room service pushes in a tray of food but Killain appears & snaps his neck. Pepper gasps.) I'm trying to fix this thing. I didn't know you and the master were gonna blow the place up. I've told you, Killian, we can use him. Look, if we want to launch product next year, I need Stark. He just lacked a decent incentive. Now, he has one. It's just like old times, huh? It wasn't my idea. I took his money. No. No, you're in a dungeon. I'm free to go. [Sighs and walks towards him] A lot has happened, Tony. But I'm close. Extremis is practically stabilised. Then help me fix it. [She shows him the \"You Know Who I Am\" note he left in 1999.] Yes. You don't remember? Let him go. I said, let him go. 1200 CCs. A dose half of this size, I'm dead. If I die, Killian, what happens to your soldiers? What happens to your product? What happens to you? What happens if you go too hot? [He looks back at Tony and shoots Maya]\n",
      "\n",
      "\n",
      "Military Aide\n",
      "Sir, we tracked the broadcast signal. We have a possible point of origin in Pakistan and the Patriot is ready to strike. Yes, sir. Everything all right, Colonel? [Iron Patriot kills the aide and people begin shooting at him. He kills everyone beside the President. The helmet lifts up to reveal Savin.\n",
      "\n",
      "\n",
      "Mrs. Davis\n",
      "Free country. Alright. Where'd you like to start? Look, I brought your damn file. You take it and go. [she drops the file in front of him] Whatever was in here, he wanted no part of it. [Tony opens the file] Yeah. [Tony looks at the file again and notices a photo of Taggart next to the photo of Chad] What? You're not the person who called me after all, are you? [suddenly a cell phone is slammed on their table]\n",
      "\n",
      "\n",
      "News Reporter #1\n",
      "And now that we seem to be back, let's recap some of the frightening...\n",
      "\n",
      "\n",
      "News Reporter #2\n",
      "American Airwaves were highjacked...\n",
      "\n",
      "\n",
      "News Reporter #3\n",
      "The nation remains on high alert...\n",
      "\n",
      "\n",
      "News Reporter #4\n",
      "All attempts to find the Mandarin have so far proved unsuccessful...\n",
      "\n",
      "\n",
      "Nurse\n",
      "[Happy wakes up, gasping and coughing.] It's okay. It's okay. It's okay.\n",
      "\n",
      "\n",
      "Officer\n",
      "Sir, Air Force One has been compromised. Internal shots, temperature spikes. Image coming through now, sir.\n",
      "\n",
      "\n",
      "Officer #2\n",
      "Get me eyes on it now. Was that Rhodes?\n",
      "\n",
      "\n",
      "Party Guest\n",
      "Tony Stark? Great speech, man!\n",
      "\n",
      "\n",
      "Pepper Potts\n",
      "Uh-huh. So, you're suggesting that I replace the entire janitorial staff with robots? What!? [Hogan points to his badge] Did you just say that? Happy? Okay, I am thrilled that you're now the Head of Security, okay? It is the perfect position for you. However... Since you've taken the post... We've had a rise in staff complaints of three hundred percent That's not a compliment. I... Yes? Thank you. Happy, we'll talk about this later. But right now I have to go deal with this very annoying thing. I used to work with him, and he used to ask me out all the time. So it's a little awkward. Killian? God, you look...you look great. I... I...I can't... What on earth have you been doing? Happy, it's okay. We're good. Yes. Stand down. Thank you. It's very nice to see you, Killian. What? What is that? Wow. It would be incredible. Unfortunately, to my ears it also sounds highly weaponizable. As in enhanced soldiers, private armies, and Tony is not... It's gonna be a no, Aldrich. As much as I'd like to help you. That's very deep. And I have no idea what it means. Happy... Yes. I just um... God, I forgot my... other thing, so... I'm just gonna... I'm sorry I'm late. I was... What the...? What is that?! You're wearing this in the house now? What is that, like Mark 15? Oh, and you have to wear your hobby in the living room? Yes, I did. I...I don't know how I could have missed that Christmas present. Is it gonna fit through the door? Okay. Did I like it? Wow. I appreciate the thought very much. So why don't you lift up that face mask and give me a kiss? Uh-huh. Well, why don't I run down to the garage and see if I can't find a crowbar to shimmy that thing open? I'll take my chances. This is a new level of lame. You ate without me, already? On date night? You mean you? Uh-huh. What? Aldrich Killian? What are you checking up on me? No, you're spying on me. I'm going to bed. Oh really? Well, I didn't notice that, at all. Machines. A distraction. I'm gonna take a shower. And you're gonna join me. Tony! Tony! Tony! Tony... I'm going to sleep downstairs. Tinker with that. Tony, is somebody there? [Tony steps out of his suit] I'm sorry. With Happy in the hospital, I didn't know we were expecting guests. And old girlfriends! That's how you did it, isn't it? Yep. Well, you know... You have saved yourself a world of pain. Trust me. [to Tony] We're going out of town. Yep! Immediately and indefinitely! Tony, this is how normal people behave. Sadly, that...is very normal. Calm down! I'm aware of that. I don't like it! Tony, we are leaving the house; that's not even up for discussion. I got you. Oh my God. Tony! [back inside as the house is being destroyed around Tony] Tony! Why were you at the house tonight? What was so important that you had to speak to Tony? Your boss works for the Mandarin, you think? But Tony says you're a botanist, so... This boss of yours, does he have a name? You can't be too hard on yourself, Maya. I mean, you gave your research to a think tank. That's exactly what we used to do. So, don't judge yourself. Maya, run! [Killian grabs Pepper by the arm, spins her around and slams her against the wall, holding her by the neck. [Breathing heavily.] You think he's gonna help you? He won't. Trophy [Gasps, panting.] Stop! Put it down. Put it down. Put it down. You're such a jerk. Tony. Oh, my God. That was really violent. I was dead. Why? Because I fell 200 feet? Who's the hot mess now? You know, I think I understand why you don't want to give up the suits. What am I going to complain about now? No, don't touch me. No, I'm gonna burn you. Am I gonna be okay? And all your distractions? It'll do.\n",
      "\n",
      "\n",
      "Pepper's Assistant\n",
      "Excuse me. Miss Potts, your four o'clock is here.\n",
      "\n",
      "\n",
      "President Ellis\n",
      "Central to my Administration's response to this terrorist event, is a newly minted resource. I know him as Colonel James Rhodes, the American people will soon know him as the Iron Patriot. I have to make this call. This is the right thing to do. [The phone rings but Mandarin doesn't answer. He waits for a few seconds before firing the gun.] Tell Rhodes, find this lunatic right now. Right now. Colonel Rhodes. [Salutes] Glad to see you could make it, son. I feel safer already. If you're gonna do it, do it! This is the Roxxon Norco. What do you want from me? Hey! What do you mean \"ready\"? [Rhodey blasts off and the President screams.]\n",
      "\n",
      "\n",
      "Pushy Tabloid Reporter\n",
      "Hey, Mr. Stark! When is somebody gonna kill this guy? Just sayin'. [Tony turns to face the reporter]\n",
      "\n",
      "\n",
      "Rhodey\n",
      "Don't move! (His phone rings.) Uh... Hang on a second. Hello? Yeah, I've had that. Who is this? A little knock-and-talk, making friends in Pakistan. What are you doing? Yeah. It's the same as it's always been, \"WarMachine68.\" Well, look, I gotta change it every time you hack in, Tony. \"WAR MACHINE ROX\" with an \"X,\" all caps. Copy. [Repulsor firing, people shrieking] Nobody move. [Women exclaiming in fear] Oh. Support Blue-Zero, unless the Mandarin's next attack on the U.S. involves cheaply-made sportswear, I think you messed up again. Yes, you're free, uh, if you weren't before. It's... Of course. Yes, ma'am. Iron Patriot on the job. Happy to help. No need to thank me, ma'am. It's my pleasure. [One of the women shakes Rhodey's hand and her hand glows red. Rhodey falls backwards, grunting.] If you want this suit, you're going to have to pry my cold dead body out of it. [Panting and sweating] Do not open. Do not open. Don't open. Don't open. [Beeping.] All right. Let's go. [The suit opens, Rhodey jumps and punches Savin in the face, then kicks him. Killian glows red and breathes fire at Rhodey.] You... You breathe fire? Okay. [Savin knocks Rhodey unconcious.] Tony? No. You got yours'? You, you, you! Move! Get out! What's this? I had winners. [Gunshots. Trevor wakes up and begins drinking beer again.] You make a move, and I break your face. This is the Mandarin? You tell him where Pepper is and he'll stop doing it. Do you know what they did to my suit? Tony, I swear to God, I'm gonna blow his face off. Yeah, a little bit. What are we gonna do? I mean, we don't have any transport. But we also have to figure out this vice president thing, right? Sir, this is Colonel Rhodes. They're using the Iron Patriot as a Trojan horse. They're gonna take out the president somehow. We have to immediately alert that plane. Rhodes and Stark out. We gotta make a decision. We can either save the president, or Pepper. We can't do both. Give me some good news, man. Oh, thank God. You couldn't save the president with the suit, how are we gonna save Pepper with nothing? [Climbs on to the boat.] Come on. You're not gonna freak out on me, right? Oh, my God. [Sighs] He's strung up over the oil tanker. They're gonna light him up, man. Yeah, death by oil. ls your gun up? Stay on my six, cover high and don't shoot me in the back. Yeah, you really killed the glass. They're not universal, Tony. Okay I don't have one that fits that gun. What'd you see? God, I would kill for some armour right now. Yeah, a bunch. Is that...? Are those...? This is how you've been managing your down time, huh? Oh, yeah. That's awesome. Give me a suit, okay? What does that mean? Very funny. Mr President! Just hold on, all right? I'm coming. Just hold on. Hold on. [Gunfire] Oh, kay. Bye-bye. Brace yourself. You look damn good, Mr President, but I'm gonna need that suit back. The President is secure, Tony. I'm clearing the area. Ready, sir?\n",
      "\n",
      "\n",
      "Rose Hill Christmas Tree Shopper\n",
      "Yeah.\n",
      "\n",
      "\n",
      "Rose Hills Sheriff\n",
      "Hey, hey, hey! What's all this about? What the hell's going on here? Yes ma'am, it is. And you are? No, we're not good. I need a little more information than that. Yeah, well, why don't you get on the horn to Nashville and uh...upgrade me? [Tony gestures to Mrs. Davis to hide the file, she pushes it under the bar] Deputy, get this woman and... [suddenly Brandt shoves the hot badge into the Sheriff's face, she takes his gun and shoots him, Tony runs out of the bar and Brandt follows him, he turns to her]\n",
      "\n",
      "\n",
      "Savin\n",
      "Merry Christmas. Can you regulate? Are you sure about that? It's a decent batch. Don't say I never did nothin' for you. What are you doin', buddy? You out by yourself? A little date night? Watching your favorite chick flick maybe? No kidding? That doesn't belong to you. [Savin goes to take the item from Hogan's hand, but Hogan punches Savin in the face twice, Hogan notices Savin's face glow red and heal, then Savin grabs hold of Hogan and with super strength throws him aside, as Savin walks towards Hogan, Taggert begins to glow red and turn super hot] What? Well, we took the house down, sir... But there's no sign of a body. No Stark. Help me! Help me! [as Tony's leg is trapped under some rubble, Savin sits in front of him with Harley sat on his lap] Hey kid, what would you like for Christmas? Oh. No, no. I think he was trying to say, \"I want my god damn file.\" Okay, Trevor, what did you tell him? Nothing? You should have pressed the panic button. Well, that's great, but the last time I looked there was somebody inside of it. [Whirring] You'll damage the armour. She's still in Phase Two. It is an honour, Mr President. Whoa! Cool your boots, sir. That's not how the Mandarin works. He's not here. [Savin burns the Iron Man suit's arm. Electrical crackling, mechanisms grinding, soft groaning.] Try the jet stream? Speaking of which, go fish. [A door blasts open on the plane, people screaming, metal creaking, more screaming. Tony blasts Savin in the chest.]\n",
      "\n",
      "\n",
      "Taggert\n",
      "Yes, I can regulate. Yes. Thank you...I mean for understanding. Savin! Help! Help me! [Taggert suddenly explodes, causing a massive explosion inside the theater which wounds Hogan, as he lies injured, Hogan sees Savin, also glowing red and then healing and walking off as if nothing has happened]\n",
      "\n",
      "\n",
      "The Mandarin\n",
      "Some people call me a terrorist, I consider myself a teacher. America, ready for another lesson. In 1864 in Sand Creek Colorado the U.S. military waited till the friendly Cheyenne braves all gone hunting, waited to attack and slaughter their families left behind, and claim their land. Thirty-nine hours ago the Ali Al Salem Air Base in Kuwait was attacked. I...I...I did that. A quaint military church filled with wives and children, of course. The soldiers were out on maneuvers, the braves were away. President Ellis, you continue to resist my attempts to educate you, sir. And now, you've missed me again. You know who I am, you don't know where I am, and you'll never see me coming. True story about fortune cookies. They look Chinese, they sound Chinese, but they're actually an American invention. Which is why they're hollow, full of lies, and leave a bad taste in the mouth. My disciples just destroyed another cheap American knock-off, The Chinese Theater. Mr. President, I know this must be getting frustrating, but this season of terror is drawing to a close. And don't worry, the big one is coming; your graduation. Well then. What are we waiting for? Mr President. Only two lessons remain and I intend to finish this before Christmas morning. Meet Thomas Richards. [The screen zooms out to show a man lying on the floor while Mandarin points a gun at him] Good strong name. Good strong job. Thomas, here, is an accountant for the Roxxon Oil Corporation. [Thomas sobs] But I'm sure he's a really good guy. I'm going to shoot him in the head, live on your television in 30 seconds. The number for this telephone is in your cell phone. Exciting, isn't it, imagining how it got there? America, if your president calls me in the next half-minute, Tom lives. Go! There's just one lesson left, President Ellis. So run away, hide, kiss your children goodbye. Because nothing, not your army, not your red, white and blue attack dog, can save you. I'll see you soon. Well, I wouldn't go in there for 20 minutes. [He laughs] Now, which one of you is Vanessa? Ah! Nessie. Did you know that fortune cookies aren't even Chinese? They're made by Americans, based on a Japanese recipe. Bloody hell. Bloody hell. I'm not moving. You want something? Take it. Although the guns are all fake because those wankers wouldn't trust me with the real ones. Hey, do you fancy either of the birds? Whoa, whoa, whoa. He's here. He's here, but he's not here. He's here, but he's not here. It's complicated. Hey, it's complicated. It's complicated. My name is Trevor. Trevor Slattery.\n",
      "\n",
      "\n",
      "Thomas Richards\n",
      "No!\n",
      "\n",
      "\n",
      "Tony Stark\n",
      "We create our own demons. Who said that? What does that even mean? Doesn't matter, I said it cause he said it. So now he was famous and that's basically get said by two well known guys. I don't, uh... (Sighs) I'm gonna start again. Let's track this from the beginning. Hey, do you want...? I gave a speech? How was it? Really? It's my favorite kind, a winning combo. Uh, to town on each other, probably back in your room. Cause I also wanna see your research. Oh, I finally met a man called \"Ho.\" [to Wu] Oh, this guy. Hey. (Dr. Wu greets Tony in mandarin as they shake hands.) You're a heart doctor. She's going to need a cardiologist after I... (Tony turns, starts blowing on his party horn & walks away with Maya.) It started in Bern, Switzerland, 1999. I never thought they'd come back to bite me. Why would they? [As Tony and Maya walk towards the elevator an enthusiastic man with long hair comes up towards them] Who isn't? He means me. Oh, wow. He made it. He made the cut. Uh... she'll take both. One to throw away and one to not call. I see that, cause it's on your t-shirt. (to the party of women in the elevator; referring to Hogan as they walk out) Ladies, follow the mullet.(To Maya) Ladies first. I'm titillated by the notion of working with you. Yeah, cheese clown. I'll see you up on the roof in five minutes. [steps out of the elevator] I'm just going to try to get my beef wet real quick. You know what I'm talkin' about? Damn betcha. Come on! I thought that was just a theory. Wow. That's incredible. Essentially you're hacking into the genetic... ...of a... ...living organism. Wow. She's not like the others. (to Maya) Come on. Let's go in the bedroom. Happy... Leave her ficus alone. And you're starting with plants? Huh. It's revolutionary. Changes the world. You're the most gifted woman I've ever met. In Switzerland. This week. You almost bought it, didn't you? Have you checked the telomerase algorithm? We're good. You're...you're right on me. I made it. (Happy gets up) What the hell was that? She was just talking about it. Glitches happening. Hey! Happy New Year. Alright, I'll see you in the morning, goodnight. Yep. Okay. [voice over] So why am I telling you this? Because I had just created demons, and I didn't even know it. [voice over] Yeah, those were the good times. Then I moved on. After a brief soiree in an Afghan cave, I said goodbye to the party scene. Forgot that night in Switzerland. These days I'm a changed man, I'm different now. I'm well... you know who I am. Ow! No. Forty-eight. [he injects himself] Ah! Micro-repeater implanting sequence complete. Which I will. Right, let's do this. Dummy. Hi, Dummy. How did you get that cap on your head? You earned it. Hey. Hey! What are you doing round in the corner? You know what you did. Blood on my mat, handle it. Focus up, ladies. Good evening, and welcome to the birthing suite. I'm pleased to announce the imminent arrival of your bouncing, bad-ass, baby brother. Start tight and go wide, stamp in time. Mark 42 autonomous prehensile propulsion suit test. Initialize sequence. Jarvis, drop my needle. Crap. Alright, I think we got this. Send 'em all. Probably a little fast, slow it down. Slow it down just a... ...little bit. Cool it, will you, Jarvis? Come on. I ain't scared of you. I'm the best. [voice over] And I guess seventy-two hours isn't a long time between siesta's. Didn't think it could get any worse. Then I had to go and turn on the TV. [voice over] That's when he happened. [putting on a mocking voice] I am Iron Patriot... So what's really goin' on? With Mandarin. Seriously, can we talk about this guy? Nine. You know I can help, just ask. I got a ton of new tech, I got a prehensile, I got a...I got a new bomb disposal. Catches explosions mid-air. Einstein slept three hours a year. Look what he did? You're gonna come at me like that? If Richard doesn't mind. [to Rhodes] You alright with this, Dick? [To the girl] What's your name? I loved you in A Christmas Story, by the way. It's not superhero business, I get it. That's why I said I...got it. I broke the crayon. What'd he say?! Sorry. Have to check on the suit...make sure...okay. Check the heart, check the...check the...is it the brain? Okay, so I was poisoned? Me? Sorry, I gotta split. Is this forehead of Security? What? Harassing interns? What's going on? Fill me in. Yeah. Right. Oh, Yeah, yeah. You're the best. Um...Switzerland. Killian? No, I don't remember that guy. His what? Look at what? You watching them? Flip the screen and then we can get started. Flip the screen, then I can see what they're doing. Relax. I'm just asking you to secure the perimeter. Tell him to go out for a drink or something? A giant brain? I miss you, Happy. Hey, I...I'd hate to cut you off. Do you have your taser on you? I think there's a gal in HR who's trying to steal some printer ink, you should probably go over there and zap her. Uh...yeah. Something like that. You know everybody needs a hobby. Just breakin' it in. You know, it's always a little pinchy in the gooey bag at first, so. Oh hey, did you see your Christmas present? Well actually, uh...it's a good question. I got a team of guys comin' tomorrow, they're gonna blow out that wall. So, uh...tense? Good day? Ooh shoulders, a little knotty. Naughty girl. I don't wanna harp on this, but did you like the custom rabbit? Nailed it, right? Huh. Yup, dammit. No can do. You wanna just kiss it on the... The facial slit? Crowbar. Yeah. Oh, except there's been a...uh...a radiation leak. That's risky. At least let me get you like a Hazmat suit or a Geiger counter or something like that. Busted. Sorry. [referring to Mark 42 suit] He was just... Well, yeah. I just mean we were just...just hosting you while I finished up a little work. And yes, I had a quick bite. I didn't know if you were comin' home or if you were having drinks with Aldrich Killian. What? Happy was concerned. I wasn't... Hold on. Come on. Pep. Hey, I admit it! My fault. Sorry. I'm a piping hot mess. It's been going on for a while, I haven't said anything. Nothing's been the same since New York. You experience things and then they're over and you still can't explain 'em. Gods, aliens, other dimensions. I...I'm just a man in a can. The only reason I haven't cracked up is probably because you moved in. Which is great. I love you, I'm lucky. But, honey, I can't sleep. You go to bed, I come down here. I do what I know, I tinker. But threat is imminent, and I have to protect the one thing that I can't live without. That's you. My suits, they're uh... They're part of me. Maybe. Okay. Better. Power down! I must have called it in my sleep. That's not supposed to happen. I'll recalibrate the sensors. Can we just...just let me...just let me catch my breath, okay? Don't go, alright? Pepper? Hi. [referring to the TV] Uh...mind leaving that on? Sunday night's PBS 'Downtown Abbey'. That's his show, he thinks it's elegant. [he pauses for a moment] One more thing...make sure everyone wears their badges. He's a stickler for that sort of thing, plus my guys won't let anyone in without them. [Tony turns to leave] Is that what you want? Here's a little Holiday greeting I've been wanting to send to the Mandarin. I just didn't know how to phrase it until now. My name is Tony Stark and I'm not afraid of you. I know you're a coward, so I've decided [He removes his sunglasses and stares into the phone's camera] that you just died, pal. I'm gonna come get the body. There's no politics here; it's just good old-fashioned revenge. There's no Pentagon; it's just you and me. And on the off-chance you're a man, here's my home address: 10-8-80, Malibu Point, 9-0-2-6-5. I'll leave the door unlocked. [to the reporter] That's what you wanted, right? [He takes the phone and throws it at a wall] Bill me. [Tony gets in his car and drives off] Okay, what do we got here? His name is an ancient Chinese war mantle, meaning \"adviser to the King\". South American insurgency tactics, talks like a Baptist preacher. There's lots of pageantry going on here...lots of theater. [Tony pushes the virtual information down to close] Close. [Tony looks at the virtual crime scene reconstruction] No bomb parts found in a three mile radius of the Chinese theater. Talk to me, Happy. [in the virtual reconstruction, Tony sees Hogan pointing his finger at some dog tags] When is a bomb not a bomb? [Tony picks up the virtual image of the dog tags to investigate them further] When is a bomb not a bomb? [Tony picks up the virtual image of the dog tags to investigate them further] Any military victims? Bring up the thermogenic signatures again, factor in three thousand degrees. Take away everywhere that there's been a Mandarin attack. [Tony looks at the information popping up] Nope. [he sees an attack in Rose Hill, Tennessee] That. You sure that's not one of his? Bring her around. That's two military guys. Ever been to Tennessee, Jarvis? Are we still at \"ding-dong\"? We're supposed to be on total security lock down. Come on, I threatened a terrorist. Who is that? Right there's fine. [Tony dressed in his Iron Man suit walks toward her] You're not the Mandarin, are you? Are you? Don't take it personally, I don't remember what I had for breakfast. That's right. Normally, I'd go for that sort of thing, but now I'm in a committed relationship. [as he turns to walk into the living room, two bags are thrown down from the above balcony] It's...with her. Yeah, it's Maya Hansen. [Maya smiles] Old botanist pal that I used to know, barely. [as Pepper starts walking downstairs, Tony moves towards Maya and speaks quietly] Please don't tell me that there is a twelve year-old kid waiting in the car that I've never met. What...what for? Why now? I'll be fine. We weren't. She's not really. Yep. It was a great night. What? Okay, we've been through this. Nope. The man says no. Honey... I'm sorry. That's a terrible idea. Please don't touch her bags. I can't protect you out there. I challenged... [Maya notices giant stuffed rabbit Tony had bought for Pepper] Yes, this is normal! It's a big bunny, relax about it! I got this for you. You still haven't even told me that you liked it! I asked you three... You don't like it?! [as Tony and Pepper are bickering, Maya notices on the TV that helicopters are coming toward Tony's home] I said no. What? I got you first. Like I said, we can't stay here. [the helicopters starts shooting at the house] Move! I'm right behind! [as they run to get out, the floor between Pepper and Tony collapses] Get her, I'm gonna find a way around. [Pepper hesitates] Stop stopping! Get her, get outside! Go! [Pepper manages to grab Maya and use the Iron Man suit to get them safely out of the house, but the house gets further destroyed as the helicopters continue to fire on it] Jarvis, where's my flight power?! That's one. That's two. [as the helicopter explodes it crashes into the house, the remaining helicopter continues on shooting at the house, finally the remains of house with Tony inside fall into the ocean, Pepper runs to the edge of the cliff and looks down into the ocean. In the water, Tony starts to feel like he's drowning] Alright, kill the alarm. I got it. It's snowing, right? Where are we, upstate? Why?! Jarvis! Not my idea! What are we doing here? This is thousands of miles away, I gotta get Pepper, I gotta... Who asked you? Open the suit. Open eject. [the suit gets off Tony and he sits up] That's brisk! [as Tony starts to feel the cold weather.] Maybe I'll just cozy back up for a bit. Jarvis. Jarvis? Don't leave me, buddy. [Tony drags the suit to a petrol station and makes a call to Pepper] Pepper, it's me. I've got a lot of apologies to make and not a lot of time. So first off, I'm so sorry I put you in harm's way. That was selfish and stupid and it won't happen again. Also, it's Christmas time, the rabbit's too big. Done. Sorry. And I'm sorry in advance because...I can't come home yet. I need to find this guy. You gotta stay safe. That's all I know. I just stole a poncho from a wooden Indian. [Tony drags the suit to an abandoned looking farmhouse, he places the suit on a couch and sits next to it] Let's get you comfy. [he places its hand by its side] You happy now? [as Tony tries to fix the micro-repeater implants in his arm, a boy stands by the doorway and points his toy gun at Tony] You got me. Nice potato gun. Barrel's a little long. Between that and the wide gauge, it's gonna diminish your FPS. [the boy points his gun at a glass on a shelf and shoots at it, breaking the glass] And now you're out of ammo. It's a electromagnet. You should know, you've got a box of them right here. Technically, I am. A valid point. Life. I built him, I take care of him, I'll fix him. Yeah. [Interupting Harley] It's Iron Patriot now. No, it's not. Retroreflective panels? You want a stealth mode? That's actually a good idea. Maybe I'll build one. Not a good idea! What are you doing? You're gonna break his finger? He's in pain, he's been injured. Leave him alone. Are you? Don't worry about it, I'll fix it. So, uh, who's home? Mm. Which happens, dads leave. No need to be a pussy about it. Here's what I need: a laptop, a digital watch, a cell phone, the pneumatic actuator from your bazooka over there, a map of town, a big spring, and a tuna fish sandwich. Salvation. What's his name? The kid that bullies you at school, what's his name? I got just the thing. [he opens a compartment on the suit and takes out a metal object] This is a pinata for a cricket. I'm kidding, it's a very powerful weapon. Point it away from your face, press the button on top. It discourages bullying. Non-lethal, just to cover one's ass. [Harley reaches out to take it, but Tony pulls his hand away] Deal? Deal? What'd you say? Deal? [Tony gives the object to Harley] What's you're name? The mechanic. Tony [Tony looks at Harley for a moment] You know what keeps going through my head? Where's my sandwich? [back at Tony's house, which is now surrounded by emergency rescue and news reporters, Pepper stands alone and looks out across the ocean. She sees one of Tony's shattered Iron Man helmets and pulls it towards her, her forehead resting against the helmet's. She sees a red flashing light inside the helmet and as she puts the helmet on she receives the message Tony had left her earlier] Pepper, it's me. [Pepper gasps and smiles] I've got a lot of apologies to make and not a lot of time. So first off, I'm so sorry I put you in harm's way. That was selfish and stupid and it won't happen again. The sandwich was fair, the spring was a little rusty, the rest of the materials, I'll make do. By the way, when you said your sister had a watch, I was kinda hoping for something a little more than that. [he pulls his sleeve up and we see he's wearing a little girl's pink watch, Harley laughs] Maybe never, relax about it. I don't know, later. Hey kid, give me a little space. [they stop and Tony looks at the remains of the local explosion site that Tony came to investigate] What's the official story here? What happened? Six people died, right? Including Chad Davis? Yeah. That doesn't make sense. [he sits next to Harley] Think about it. Six dead, only five shadows. Do you buy that? No idea. I'm not...I don't care. That's manipulative. I don't want to talk about it. Maybe. Can you stop? Remember when I told you, that I have an anxiety issue? Yeah, a little bit. Can I just catch my breath for a second? Nope. Probably. I don't think so. Remember when I said to stop doing that? I swear to God, you're going to freak me out! [Tony, looking agitated, suddenly rises] Ah man, you did it, didn't you? You happy now? Your fault, you spazzed me out. Okay, back to business. Where were we? The guy who died...relatives? Mom? Mrs. Davis, where is she? See, now you're being helpful. [later we see Tony walking towards a bar and he bumps into a woman] Sorry. [the woman drops something] Lady, this uh... [he picks up the item and hands it back to the woman] Nice haircut, suits you. Yeah, limited edition. Mrs. Davis, mind if I join you? It sure is. [Tony sits next to her, Mrs. Davis looks at Tony for a moment] I just want to say I'm sorry about your loss. I want to know what you think happened. Clearly, you're waiting for someone else. Yeah? Supposed to meet somebody here? Mrs. Davis, your son didn't kill himself, I guarantee you. He didn't kill anyone. Someone used him. As a weapon. Hey hot wings, you wanna party? You and me, let's go. [as Tony turns to run again he sees Savin getting out of a car and walk towards him, as Savin gets his gun out to shoot at him, Tony runs off and Harley throws something at Savin to make him miss Tony, Tony stops behind a car and sees a man hiding low on the ground] Crazy, huh? Watch this. [Tony smashes into the window of a shop] [as Tony and Brandt are fighting in the shop, Tony starts a fire in shop] You walked right into this one, I've dated hotter chicks than you. [Tony puts Chad's dog tags into the microwave and he turns on the gas] Sweetheart, that could be the name of my autobiography. [Tony quickly leaves through the back door, as the dog tags heat up in the microwave they start sparking up fire, Brandt realizes the gas has been left on and suddenly the shop explodes killing Brandt] [Tony stumbles out of the shop as people run past. A water tank falls down and traps Tony's leg. He finds that Savin has grabbed Harley] It's not your fault, kid. Remember what I told you about bullies. [Harley uses the gadget Tony gave him earlier and runs away] Do you like that, Westworld? That's the thing about smart guys, we always cover our ass. [Tony brings up his hand and reveals his Iron Man suit hand. He fires at Savin and sends him flying backwards. Tony makes his way out of the rubble before leaving.] For what? Did I miss something? Yeah. A, saved you first. B, thanks, sort of. And C, if you do someone a solid, don't be a yutz, alright? Just play it cool otherwise you come off grandiose. What I need is for you to go home, be with your mom, keep your trap shut, guard the suit and stay connected to the telephone because if I call, you better pick up. Okay, can you feel that? We're done here. Move it out the way or I'm going to run you over. Bye kid. [Tony gets in his car] I'm sorry, kid, you did good. Yep [A beat] Wait, you're guilt tripping me, aren't you? [Mocking him] I can tell. You know how I can tell? Cus we're connected. [Tony drives away] Man. Happy, Happy, Happy- You ever have a chick straddling you and you look up and suddenly she's glowing from the inside out, kind of a bright orange? It's me, pal. Now, last time I went missing, if I remember correctly, you came looking for me. What are you doing? Your redesign, your big rebrand, that was AIM, right? I'm gonna find a heavy-duty comm sat right now, I need your login. And password, please. It's not the '80s, nobody says \"hack\" any more. Give me your login. (Rhodey sighs.) (He's laughing.) That is so much better than \"lron Patriot.\" That ain't gonna cut it. Shh. Shh. Keep it down. No, he's not. Come on in. Close the door. (The cameraman gets in the van & closes the door.) Shh... Yep. Okay. First, is this your van? Is anyone else gonna come in? Great. What's your name? Gary? Right there is fine. Okay? I get a lot of this, it's okay. What do you want? It's fine. Right. A Hispanic Scott Baio. I'm sorry. Is that me? Gary. Listen to me, okay? I don't want to clip your wings, here. We're both a little over-excited. I got an issue. I'm chasing bad guys. I'm trying to grab a little something from some hard-crypt data files. I don't have enough juice. I need you to jump on the roof... Right? Recalibrate the lSDNs. Pump it up by about 40%. All right? It's a mission. Tony needs Gary. Be quiet about it. Go. (He's muttering.) A bomb is not a bomb when it's a misfire. The stuff doesn't always work. Right, pal? It's faulty, but you found a buyer, didn't you? Sold it to the Mandarin. Got you, pal. Harley, tell me what's happening. Give me a full report. How much have you had? Can you still see straight? That means you're fine. Give me Jarvis. Jarvis, how are we? What are we talking? Far East, Europe, North Africa, Iran, Pakistan, Syria? Where is it? Okay, kid, I'm gonna have to walk you through rebooting Jarvis's speech drive, but not right now. Harley, where is he really? Just look on the screen and tell me where it is. Okay, first things first, I need the armour. Where are we at with it? What's questionable about electricity? All right? It's my suit, and I can't... I'm not gonna... I don't wanna... [Breathing heavily] Oh, God, not again. Right, and then you just said it by name while denying having said it. [Panting] God, what am I gonna do? Right. Yes, I did. Okay. Thanks, kid. [He gets back in the car.] [Cut to Tony in a shop buying supplies.] [Cut to Tony building something.] [Cut to Miama, Florida. Tony climbs a wall and begins fighting guards. He makes his way into the house and finds a woman lying on a table.] [Cut to a different woman lying on a sofa with a man sitting at a desk.] Hey! [Mandarin raises his hands] Don't move. What? Heard enough. You're not him. The Mandarin, the real guy. Where? Where's the Mandarin? Where is he? What do you mean? It is? Uncomplicate it. Ladies, out. Get out of the bed. Get into the bathroom. Sit. [Door closes. Gunshot. Women exclaim in fear] What are you? What are you, a decoy? You're a double, right? You got a minute to live. Fill it with words. Then how did you get here, Trevor? Next? What did they say? They'd get you off them? Did you just nod off? Hey. [Tony kicks him] \"He\"? Killian? He created you? Custom-made terror threat. Your performance? Where people died? I'm sorry, but I got a best friend who's in a coma and he might not wake up. So you're gonna have to answer for that. You're still going down, pal. You under... [Savin hits Tony around the head, unconcious] Ah... [Sighs] Okay. Oh, yeah. With zip ties. It's a ball. Okay. So you took Killian's card. And here you are 13 years later, in a dungeon. Yeah. Yeah? [Shouting] I'm telling you it isn't. [Lowers voice to usual volume] I'm on the street. People are going bang. They're painting the walls. Maya, you're kidding yourself. Did I do that? I remember the night, not the morning. Is this what you've been chasing around? I can't help you. You used to have a moral psychology. You used to have ideals. You wanted to help people. Now look at you. I get to wake up every morning with someone who still has their soul. Get me out of here. Come on. You're not still pissed off about the Switzerland thing, are you? Honestly, I'm still trying to figure out what happened to the first mouse. You're something else. Yes. Sir Laurence Oblivier. What's next for you in your world? You are a maniac. [Watch beeping.] Careful, there. It's a limited edition. Hey, uh, Ponytail Express. What's the mileage count between Tennessee and Miami? Very nice. Break it, you bought it. Okay, that wasn't mine to give away. That belongs to my friend's sister. And that's why I'm gonna kill you first. You'll see. Trust me, you're gonna be in a puddle of blood on the ground in five, four, three... Come on! Two... All right, I'm gonna give you a chance to escape. Put down your weapons. Tie yourselves to those chairs. I'll let you live. In five, four, bang! You should be gone by now. You should've already been gone. Here it comes. Three, four... [Quickly] Five, four, three, two, one! [The Iron Man suit joins to his left arm] Told you. Where's the rest? [He blasts one of the men and unties himself. As they fight, the suit connects to his right leg and he kicks them] Where's the rest? [Meanwhile, Harley watches as the garage door shakes. He runs over and blows it open. The suit flies out.] [Repulsor powering up, gunfire, screaming.] Ah! Better late than never. [He gets outside and catches the face mask.] Not this time. Not the face. Phew! It's good to be back. Hello, by the way. Ah! Let's go! [Thrusters misfiring.] Aw, crap. Rhodey, tell me that was you in the suit. Uh... Mmm. Kind of. Main house, as fast as you can. There's somebody I'd like you to meet. [Girls whimpering] The room is secure. I have eyes on the Mandarin. What have you come as? I never thought people had been hurt. They lied to me. Yeah, I know, it's... It's embarrassing. Here's how it works, Meryl Streep. Spill. What? No. But I do know it's happening off the coast. Something to do with a big boat. I can take you there. Oh, and this next bit may include the vice president as well. Is that... Is that important? So? Right. [Beer opens] Hey, Ringo. Didn't you say something about a \"lovely speedboat\"? If he's right about the location, we're 20 minutes from where Pepper is. Right. I wonder who I'm calling right now. Oh! That's the vice president. Sir, this is Tony Stark. We believe you're about to be drawn into the Mandarin campaign. We gotta get you somewhere safe as soon as possible. And what about the suit I'm wearing? That's going to have to do. The President. Now. Walk away from that, you son of a bitch. [Savin falls to the ground.] How many in the air? How many can I carry? Slow down. Slow down, relax. What's your name? Heather? Listen to me. See that guy? I'm gonna swing by, you're just gonna grab him. You got it? I'll electrify your arm, you won't be able to open your hand. We can do this, Heather. Easy, see? Eleven more to go. Remember that game called Barrel of Monkeys? That's what we're going to do. Come on, people. Everybody, grab your monkey. Nice. Come on, people. Come on, come on, come on! Yeah! He's a chunky monkey, let's get him. Hello. [Tony drops them all in the water from a few feet above.] Nice work, guys! Excellent. Good team effort all around. Go us. All right, Jarvis. But it's only half-done. We've still got to get Pepper... [The Iron Man suit flies into a truck and the suit disassembles. He sighs.] That came out of nowhere. Wow [We now see Tony on the boat, where he was controlling the suit.] I think they all made it. Yeah, but I missed the president. Uh... Say, Jarvis, is it that time? Correct. I hope not. Viking funeral. Public execution. Yep. What do I do? Six, high, back. Alright. [Gunfire, bullets ricocheting] You see that? Nailed it. You think I was aiming for the bulb? You can't hit a bulb at this distance. [Rhodey fires a bullet and hits the bulb.] I'm out. Give me. You got extra magazines? I know what I'm doing, I make this stuff. Give me another one. Give me one of yours. You've got, like, five of them. Here's what I'm going to do. Save my spot, ready? Too fast. Nothing. Here we go. [Clears throat] Three guys, one girl, all armed. You're right. We need backup. You know what? [He nods at something in the distance, whooshing] Yep. Yeah. [Dozens of Iron Man suits fly over.] Merry Christmas, buddy. JARVIS, target Extremis heat signatures. Disable with extreme prejudice. What are you waiting for? It's Christmas. Take them to church. Incoming! Jarvis, get Igor to steady this thing. [Mechanisms whirring.] Everybody needs a hobby. Heartbreaker, help Red Snapper out, will you? [Tony suits up.] Nice timing. Oh, I'm sorry, they're only coded to me. I got you covered. About time. See what happens when you hang out with my ex-girlfriends? Yep. We'll talk about it over dinner. [Metal creaking, Pepper sobbing.] Come on. A little more, baby. [Pepper straining, Tony grunting. Killian shoves his hand on Tony's suit, burning it.] Yeah, you take a minute. [Groaning, sizzling, metal creaking, yelling, grunts, Pepper whimpers.] Jarvis, give me a suit right now! [The suit he called gets destroyed.] Oh, come on! Nice work. Pep, I got you. Relax, I got you. Just look at me! Honey, I can't reach any further and you can't stay there. All right? You've got to let go. You've got to let go! I'll catch you, I promise. No! [Pepper falls to the ground and into the flames below.] Eject. [More repulsors firing.] I'll be damned. The prodigal son returns. [The suit flies to Tony, hits a piece of metal and falls apart. Rolls his eyes.] Whatever. Okay, okay, wait, wait, wait! Slow down! Slow down! You're right. I don't deserve her. Here's where you're wrong. She was already perfect. [Tony motions to the suit and it flies on to Killian.] Jarvis, do me a favour and blow Mark 42. I got nothing. [Repulsor powering up.] Jarvis, subject at my 12 o'clock is not a target, disengage! What? Oh, what, are you mad at me? [Pepper runs towards Tony, flips in the air, punches through the Iron Man suit and destroys it. She uses the suit to blow up Killian.] Honey? You just scared the devil out of me. I thought you were... It's still debatable. Probably tipping your way a little bit. Why don't you dress like this at home? Hmm? Sport bra. The whole deal. Well, it's me. You'll think of something. Come here. Don't worry about it. No, you're not. Not hot. No. You're in a relationship with me. Everything will never be okay. But I think I can figure this out, yeah. I almost had this 20 years ago when I was drunk. I think I can get you better. That's what I do. I fix stuff. Uh... I'm going to shave them down a little bit. Jarvis. Hey. You know what to do. Screw it, it's Christmas. Yes, yes. [Pepper and Tony hug as the Iron Man suits blow up.] Okay, so far? Do you like it? And so, as Christmas morning began, my journey had reached its end. You start with something pure, something exciting. Then, come the mistakes. The compromises. We create our own demons. But then I thought to myself, \"Why stop there?\" Of course, there are people who say progress is dangerous, but I'll bet none of those idiots ever had to live with a chestful of shrapnel. And now, neither will I. Let me tell you,[Metal clinking.] that was the best sleep I'd had in years. [Throws Arc Reactor into the ocean.] So, if I were to wrap this up, tie it with a bow, or whatever... I guess I'd say my armor, it was never a distraction, or a hobby. It was a cocoon. And now, I'm a changed man. You can take away my house, all my tricks and toys. One thing you can't take away... I am Iron Man. [Thrilling music playing, credit scene.] You know, and thank you by the way. For listening. Plus, something about just getting it off my chest, and putting it out there in the atmosphere, instead of holding this in... I mean, this is what gets people sick, you know. Wow, I had no idea you were such a good listener. To be able to share all my intimate thoughts and my experiences with someone, it just cuts the weight of it in half. You know, it's like a snake swallowing its own tail. Everything comes full circle. [The camera zooms out to show Bruce Banner, asleep.] And the fact that you've been able to -help me process... [Tony stops and looks over as Bruce rubs his eyes.] Are you with me? Are you actively napping? Where did I lose you? So, you heard none of it? So? What? The time? You know what? Now that I think about it... Oh! God, my original wound. 1983, all right? I'm 14 years old, I still have a nanny. That was weird.\n",
      "\n",
      "\n",
      "Trevor Slattery\n",
      "What, you mean like an understudy? No, absolutely not. [Tony points the gun at Trevor's face] Don't hurt the face! I'm an actor. It's just a role. \"The Mandarin,\" see, it's not real. Um. Well, I, um, had a little problem with, um, substances. And I ended up, um, doing things, no two ways about it, in the street, that a man shouldn't do. Then, they approached me about the role, and they knew about the drugs. They said they'd give me more. They gave me things. They gave me this palace. They gave me plastic surgery. They gave me things. [Snoring] No, and a lovely speedboat. And the thing was, he needed someone to take credit for some accidental explosions. [Mimic explosion] Killian. He created me. Yes. Yes. His think tank thinked it up. The pathology of a serial killer. The manipulation of Western iconography. Ready for another lesson? Blah, blah, blah. No. Of course, it was my performance that brought the Mandarin to life. No, they didn't. Look around you. The costumes, green screen. Honestly, I wasn't on location for half this stuff. And when I was, it was movie magic, love. I didn't tell him anything. No. Well, I panicked, but then I handled it. Hi, Trevor. Trevor Slattery. I know I'm shorter in person. A bit smaller. Everyone says that. But, um, hey, if you're here to arrest me, there's some people I'd like to roll on. Doing what? Ow, I get it! Ow! That hurt. I get it! I get it! I don't know about any Pepper, but I know about the plan. Woah! [Rhodey jumps.] Ole', ole', ole', ole'... Oh! Great to see you! Oh, bloody hell!\n",
      "\n",
      "\n",
      "Vanessa\n",
      "That's me. There's some guy over here.\n",
      "\n",
      "\n",
      "Vice President\n",
      "Oh god, not again. Is the President getting this? Thanks. Hello? Welcome back to the land of the living. Mr Stark, I'm about to eat honey-roast ham, surrounded by the Agency's finest. The president's safe on Air Force One with Colonel Rhodes. I think we're good, here. Okay, I'm on it. I'll have security lock it down. If need be, they can have F-22s in the air in 30 seconds. Thank you, Colonel. Couldn't be better. (To his daughter) I love you, babe.\n",
      "\n",
      "\n",
      "Woman\n",
      "Savin? I've acquired the Patriot armour. That's the plan, Colonel. Why is it so hot in here? I told you to put it at 68. Let us out! [Tony attacks Savin and pins him against the wall.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in speaker_dialogue_mapping_grouped.iterrows():\n",
    "    print(row['Speaker'])\n",
    "    print(row['Dialogue'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aldrich Killian</td>\n",
       "      <td>Mr. Stark! Oh, wow! Hey, Tony! Aldrich Killian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Announcer</td>\n",
       "      <td>(On PA) Broadcast will commence shortly. Take ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bill Maher</td>\n",
       "      <td>And how is President Ellis responding? By taki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both</td>\n",
       "      <td>...Genetic operating system... Human application.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brandt</td>\n",
       "      <td>Thank you. [Tony notices the woman has burn ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Speaker                                           Dialogue\n",
       "0  Aldrich Killian  Mr. Stark! Oh, wow! Hey, Tony! Aldrich Killian...\n",
       "1        Announcer  (On PA) Broadcast will commence shortly. Take ...\n",
       "2       Bill Maher  And how is President Ellis responding? By taki...\n",
       "3             Both  ...Genetic operating system... Human application.\n",
       "4           Brandt  Thank you. [Tony notices the woman has burn ma..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_dialogue_mapping_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-mcu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('archive/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8675\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFJ' 'ENTP' 'INTP' 'INTJ' 'ENTJ' 'ENFJ' 'INFP' 'ENFP' 'ISFP' 'ISTP'\n",
      " 'ISFJ' 'ISTJ' 'ESTP' 'ESFP' 'ESTJ' 'ESFJ']\n"
     ]
    }
   ],
   "source": [
    "personality_types = df['type'].unique() \n",
    "print(personality_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    regex = re.compile('[%s]' % re.escape('|'))\n",
    "    text = regex.sub(\" \", text)\n",
    "    words = str(text).split()\n",
    "    words = [i.lower() + \" \" for i in words]\n",
    "    words = [i for i in words if not \"http\" in i]\n",
    "    words = \" \".join(words)\n",
    "    words = words.translate(words.maketrans('', '', string.punctuation))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['posts'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  enfp  and  intj  moments  sportscenter  not  t...  \n",
       "1  im  finding  the  lack  of  me  in  these  pos...  \n",
       "2  good  one    of  course  to  which  i  say  i ...  \n",
       "3  dear  intp  i  enjoyed  our  conversation  the...  \n",
       "4  youre  fired  thats  another  silly  misconcep...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['posts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  enfp  and  intj  moments  sportscenter  not  t...\n",
       "1  ENTP  im  finding  the  lack  of  me  in  these  pos...\n",
       "2  INTP  good  one    of  course  to  which  i  say  i ...\n",
       "3  INTJ  dear  intp  i  enjoyed  our  conversation  the...\n",
       "4  ENTJ  youre  fired  thats  another  silly  misconcep..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'cleaned_text': 'posts'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'INFJ' 'ENTP' 'INTP' 'INTJ' 'ENTJ' 'ENFJ' 'INFP' 'ENFP' 'ISFP' 'ISTP' 'ISFJ' 'ISTJ' 'ESTP' 'ESFP' 'ESTJ' 'ESFJ'\n",
    "id2label = {0: 'INFJ', 1: 'ENTP', 2: 'INTP', 3: 'INTJ', 4: 'ENTJ', 5: 'ENFJ', 6: 'INFP', 7: 'ENFP', 8: 'ISFP', 9: 'ISTP', 10: 'ISFJ', 11: 'ISTJ', 12: 'ESTP', 13: 'ESFP', 14: 'ESTJ', 15: 'ESFJ'} \n",
    "label2id = {'INFJ': 0, 'ENTP': 1, 'INTP': 2, 'INTJ': 3, 'ENTJ': 4, 'ENFJ': 5, 'INFP': 6, 'ENFP': 7, 'ISFP': 8, 'ISTP': 9, 'ISFJ': 10, 'ISTJ': 11, 'ESTP': 12, 'ESFP': 13, 'ESTJ': 14, 'ESFJ': 15} # label rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  label\n",
       "0  INFJ  enfp  and  intj  moments  sportscenter  not  t...      0\n",
       "1  ENTP  im  finding  the  lack  of  me  in  these  pos...      1\n",
       "2  INTP  good  one    of  course  to  which  i  say  i ...      2\n",
       "3  INTJ  dear  intp  i  enjoyed  our  conversation  the...      3\n",
       "4  ENTJ  youre  fired  thats  another  silly  misconcep...      4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['type'].map(label2id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>i  loved  all  the  light  we  cannot  see  by...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>it  depends  if  i  care  about  it  i  fight ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5414</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>welcome  home  sonny  laughing  just  because ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>thats  really  cool  of  you  i  like  it  whe...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8294</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>the  duck  is  named  zeus  nope  see  traits ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  label\n",
       "4080  INFJ  i  loved  all  the  light  we  cannot  see  by...      0\n",
       "2614  ENFP  it  depends  if  i  care  about  it  i  fight ...      7\n",
       "5414  ENTP  welcome  home  sonny  laughing  just  because ...      1\n",
       "1039  ENFP  thats  really  cool  of  you  i  like  it  whe...      7\n",
       "8294  ENTP  the  duck  is  named  zeus  nope  see  traits ...      1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6940 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6940/6940 [04:34<00:00, 25.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# encode the text\n",
    "\n",
    "encoded_train_data = tokenizer.batch_encode_plus(\n",
    "    tqdm(train_df['posts'].values), \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1735 [00:00<?, ?it/s]c:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 1735/1735 [01:09<00:00, 24.87it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_test_data = tokenizer.batch_encode_plus(\n",
    "    tqdm(test_df['posts'].values), \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  3866,  ...,  2030,  8426,   102],\n",
       "        [  101,  2009,  9041,  ...,  2189,  1999,   102],\n",
       "        [  101,  6160,  2188,  ..., 27036,  1998,   102],\n",
       "        ...,\n",
       "        [  101,  4921,  2063,  ...,  3030,  3331,   102],\n",
       "        [  101,  2130,  1996,  ...,  1999,  3622,   102],\n",
       "        [  101,  2077,  3752,  ...,  2008,  2115,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_data.keys()\n",
    "encoded_train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs for BERT model\n",
    "input_ids_train = encoded_train_data['input_ids']\n",
    "attention_masks_train = encoded_train_data['attention_mask']\n",
    "labels_train = torch.tensor(train_df['label'].values)\n",
    "\n",
    "input_ids_test = encoded_test_data['input_ids']\n",
    "attention_masks_test = encoded_test_data['attention_mask']\n",
    "labels_test = torch.tensor(test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=16, output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criteria = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m     train_preds \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_true = []\n",
    "    train_iterator = tqdm(train_dataloader, desc = 'Training')\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_true.extend(labels.cpu().numpy())\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_acc = accuracy_score(train_true, train_preds)\n",
    "    train_f1 = f1_score(train_true, train_preds, average='weighted')\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print(f'Train loss: {train_loss}, Train acc: {train_acc}, Train f1: {train_f1}')\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    test_iterator = tqdm(test_dataloader, desc = 'Testing')\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            test_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "    test_loss = test_loss / len(test_dataloader)\n",
    "    test_acc = accuracy_score(test_true, test_preds)\n",
    "    test_f1 = f1_score(test_true, test_preds, average='weighted')\n",
    "    print(f'Test loss: {test_loss}, Test acc: {test_acc}, Test f1: {test_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Roberta with one linear layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('archive/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8675\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFJ' 'ENTP' 'INTP' 'INTJ' 'ENTJ' 'ENFJ' 'INFP' 'ENFP' 'ISFP' 'ISTP'\n",
      " 'ISFJ' 'ISTJ' 'ESTP' 'ESFP' 'ESTJ' 'ESFJ']\n"
     ]
    }
   ],
   "source": [
    "personality_types = df['type'].unique() \n",
    "print(personality_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    regex = re.compile('[%s]' % re.escape('|'))\n",
    "    text = regex.sub(\" \", text)\n",
    "    words = str(text).split()\n",
    "    words = [i.lower() + \" \" for i in words]\n",
    "    words = [i for i in words if not \"http\" in i]\n",
    "    words = \" \".join(words)\n",
    "    words = words.translate(words.maketrans('', '', string.punctuation))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['posts'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  enfp  and  intj  moments  sportscenter  not  t...  \n",
       "1  im  finding  the  lack  of  me  in  these  pos...  \n",
       "2  good  one    of  course  to  which  i  say  i ...  \n",
       "3  dear  intp  i  enjoyed  our  conversation  the...  \n",
       "4  youre  fired  thats  another  silly  misconcep...  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['posts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  enfp  and  intj  moments  sportscenter  not  t...\n",
       "1  ENTP  im  finding  the  lack  of  me  in  these  pos...\n",
       "2  INTP  good  one    of  course  to  which  i  say  i ...\n",
       "3  INTJ  dear  intp  i  enjoyed  our  conversation  the...\n",
       "4  ENTJ  youre  fired  thats  another  silly  misconcep..."
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'cleaned_text': 'posts'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'INFJ' 'ENTP' 'INTP' 'INTJ' 'ENTJ' 'ENFJ' 'INFP' 'ENFP' 'ISFP' 'ISTP' 'ISFJ' 'ISTJ' 'ESTP' 'ESFP' 'ESTJ' 'ESFJ'\n",
    "id2label = {0: 'INFJ', 1: 'ENTP', 2: 'INTP', 3: 'INTJ', 4: 'ENTJ', 5: 'ENFJ', 6: 'INFP', 7: 'ENFP', 8: 'ISFP', 9: 'ISTP', 10: 'ISFJ', 11: 'ISTJ', 12: 'ESTP', 13: 'ESFP', 14: 'ESTJ', 15: 'ESFJ'} \n",
    "label2id = {'INFJ': 0, 'ENTP': 1, 'INTP': 2, 'INTJ': 3, 'ENTJ': 4, 'ENFJ': 5, 'INFP': 6, 'ENFP': 7, 'ISFP': 8, 'ISTP': 9, 'ISFJ': 10, 'ISTJ': 11, 'ESTP': 12, 'ESFP': 13, 'ESTJ': 14, 'ESFJ': 15} # label rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  label\n",
       "0  INFJ  enfp  and  intj  moments  sportscenter  not  t...      0\n",
       "1  ENTP  im  finding  the  lack  of  me  in  these  pos...      1\n",
       "2  INTP  good  one    of  course  to  which  i  say  i ...      2\n",
       "3  INTJ  dear  intp  i  enjoyed  our  conversation  the...      3\n",
       "4  ENTJ  youre  fired  thats  another  silly  misconcep...      4"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['type'].map(label2id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929\n"
     ]
    }
   ],
   "source": [
    "# check max length of a value in the posts column\n",
    "max_len = df['posts'].apply(lambda x: len(x.split())).max()\n",
    "print(max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset class\n",
    "class MBTIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['posts']\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        ) \n",
    "        # print (inputs)\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.data.iloc[index]['label'], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MBTIDataset(train_df, tokenizer, max_length)\n",
    "test_dataset = MBTIDataset(test_df, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   757,  1437,   202,  1437,    10,  1437, 33799,  1437,    89,\n",
      "          1437,    32,  1437,    80,  1437,  2188,  1437,   269,  1437,     5,\n",
      "          1437,    78,  1437,    74,  1437,    28,  1437,    14,  1437,    63,\n",
      "          1437,   373,  1437,   442,  1437,   657,  1437,    13,  1437,    10,\n",
      "          1437,  1219,  1437,     8,  1437,   939,  1437, 33976,  1437,   206,\n",
      "          1437,    47,  1437,    64,  1437,   109,  1437,    24,  1437,  3099,\n",
      "          6459,  1437,  3867,  1437,   110,  1437,    11,  1437,   657,  1437,\n",
      "            61,  1437,  1437,  2088,  1437,   393,  1437,    57,  1437,  4356,\n",
      "          1437,   182,  1437, 31201,  1437,    24,  1437,  1302,  1437,    65,\n",
      "          1437,   183,  1437,  4356,  1437,    11,  1437,  3016,  4717,  1437,\n",
      "             8,  1437, 12256,  1437, 10844,  1437,   172,  1437,   220,  1437,\n",
      "          4356,  1437,    11,  1437,    70,  1437,   909,  1437,    19,  1437,\n",
      "            10,  1437, 16576,  1437,   939,  1437,   657,  1437,    70,  1437,\n",
      "          3505,  1437,     9,  1437,  3152,  1437,  1437,  1437,    63,  1437,\n",
      "            45,  1437,    14,  1437,  4356,  1437,    10,  1437, 29342,   219,\n",
      "          1437,   144,  1437,     9,  1437,   127,  1437,   964,  1437,  1701,\n",
      "          1437,   162,  1437,    65,  1437,   939,  1437,   524,  1437,     5,\n",
      "          1437,    65,  1437,    54,  1437,  1074,  1437,    11,  1437,   127,\n",
      "          1437,  1508,  1437,  9646,  1437,    31,  1437,     5,  1437,  3157,\n",
      "          1437,   939,  1437,   524,  1437,     5,  1437,    65,  1437,    47,\n",
      "          1437,   192,  1437,  3051,  1437,  4374, 12445,  1437,   198,  1437,\n",
      "          1139,  1437,    70,  1437,   363,  1437,   939,  1437,   524,  1437,\n",
      "             5,  1437,    65,  1437,    19,  1437,    10,  1437, 14061,  1437,\n",
      "            15,  1437,   127,  1437,  4117,  1437,    31,  1437,  3064,  1437,\n",
      "           150,  1437,   546,  1437,    23,  1437,     5,  1437,   363,  1437,\n",
      "          6360,  1437,   326,  4734, 39941,     2],\n",
      "        [    0,  8287,  1437,    95,  8287,  1437,    63,  1437,    45,  1437,\n",
      "           190,  1437,    14,  1437,    65,  1437, 43654,  1437,   143,  1437,\n",
      "           761,  1437,     9,  1437,  1291,  1437,    64,  1437,   173,  1437,\n",
      "           114,  1437,     5,  1437,    80,  1437,  1799,  1437,    32,  1437,\n",
      "          2882,  1437,     7,  1437,   173,  1437,    15,  1437,    49,  1437,\n",
      "         16855,  1437,   109,  1437,    45,  1437,   356,  1437,    13,  1437,\n",
      "            10,  1437,  6718,  1437,  6680,  1437,     7,  1437, 12567,  1437,\n",
      "            86,  1437,     7,  1437,   146,  1437,    10,  1437,   568,  1437,\n",
      "           549,  1437,    24,  1437,    40,  1437,  2581,  1437,    50,  1437,\n",
      "            45,  1437,  4812,  1437,   192,  1437,  2140,  1437,    77,  1437,\n",
      "            47,  1437,   489,  1437,    24,  1437,   588,  1437,  1056,  1437,\n",
      "            15,  1437,   383,  1437,    14,  1437,  1796,  1437,    47,  1437,\n",
      "            70,  1437,     5,  1437, 30914,  1437,    14,  1437,    16,  1437,\n",
      "           383,  1437,    14,  1437,    47,  1437,   802,  1437,    14,  1437,\n",
      "           948,  1437,   386,  1437, 26849,  1437,    65,  1437,    30,  1437,\n",
      "            65,  1437,  7735,  1437,    24,  1437,    16,  1437,   141,  1437,\n",
      "           383,  1437,    52,  1437,   342,  1437,    98,  1437,   203,  1437,\n",
      "          1351,  1437,    88,  1437,    25,  1437,   784,  6909,  4399,  1437,\n",
      "            25,  1437,    24,  1437,  2653,  1437,     8,  1437,    25,  1437,\n",
      "         17322,  1437,    25,  1437,    24,  1437,    64,  1437,    28,  1437,\n",
      "            70,  1437,    42,  1437,   543,  1437,   173,  1437,  4356,  1437,\n",
      "          2057,  1437,   817,  1437,   162,  1437,   619,  1437,  4299,  1437,\n",
      "         16112,  1437,   489,  1437,    24,  1437,    62,  1437, 11380,  1437,\n",
      "           939,  1437,   206,  1437,   939,  1437,    95,  1437,  2263,  1437,\n",
      "          2185,  1437,    37,   298,  1437, 33976,  1437,  4076,  1437,     7,\n",
      "          1437,  4309,  1437,    59,  1437,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([6, 5])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criteria = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClassifier(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining model architecture\n",
    "class RobertaClassifier(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(RobertaClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.fc = torch.nn.Linear(768, 16)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.fc(last_hidden_state)\n",
    "        return logits\n",
    "    \n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model = RobertaClassifier(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, criteria, epochs=1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        avg_f1_micro = 0\n",
    "        avg_f1_macro = 0\n",
    "        avg_f1_weighted = 0\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criteria(outputs, labels)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # calculate micro f1 score, macro f1 and weighted f1\n",
    "            # labels = torch.argmax(labels, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "            f1_micro = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "            f1_macro = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "            f1_weighted = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "            avg_f1_micro += f1_micro\n",
    "            avg_f1_macro += f1_macro\n",
    "            avg_f1_weighted += f1_weighted\n",
    "        \n",
    "        avg_f1_micro /= len(train_loader)\n",
    "        avg_f1_macro /= len(train_loader)\n",
    "        avg_f1_weighted /= len(train_loader)\n",
    "        train_losses.append(total_loss/len(train_loader))\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {total_loss/len(train_loader)}, Training F1 Micro: {avg_f1_micro}, Training F1 Macro: {avg_f1_macro}, Training F1 Weighted: {avg_f1_weighted}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            avg_f1_micro = 0\n",
    "            avg_f1_macro = 0\n",
    "            avg_f1_weighted = 0\n",
    "            for i, batch in enumerate(tqdm(valid_loader)):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                loss, preds = model(input_ids, attention_mask)\n",
    "                val_loss += loss.item()\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "                preds = torch.argmax(preds, dim=1)\n",
    "                f1_micro = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "                f1_macro = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "                f1_weighted = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "                avg_f1_micro += f1_micro\n",
    "                avg_f1_macro += f1_macro\n",
    "                avg_f1_weighted += f1_weighted\n",
    "            avg_f1_micro /= len(valid_loader)\n",
    "            avg_f1_macro /= len(valid_loader)\n",
    "            avg_f1_weighted /= len(valid_loader)\n",
    "            valid_losses.append(val_loss/len(valid_loader))\n",
    "            print(f'Epoch {epoch + 1}, Validation Loss: {val_loss/len(valid_loader)}, Validation F1 Micro: {avg_f1_micro}, Validation F1 Macro: {avg_f1_macro}, Validation F1 Weighted: {avg_f1_weighted}')\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/3470 [00:15<1:06:36,  1.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[203], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[202], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, valid_loader, optimizer, criteria, epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criteria(outputs, labels)\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[197], line 9\u001b[0m, in \u001b[0;36mRobertaClassifier.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m     11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(last_hidden_state)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         output_attentions,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    452\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    453\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 455\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:468\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    467\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 468\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:379\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 379\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    381\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = train(model, train_loader, test_loader, optimizer, criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Character, Personality]\n",
       "Index: []"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe with colums as character and personality\n",
    "true_personality_labels = pd.DataFrame(columns=['Character', 'Personality'])\n",
    "\n",
    "# add elements to the dataframe\n",
    "true_personality_labels = true_personality_labels.append({'Character': 'Tony Stark', 'Personality': 'ENTP'}, ignore_index=True)\n",
    "true_personality_labels = true_personality_labels.append({'Character': 'Pepper Potts', 'Personality': 'ISTJ'}, ignore_index=True)\n",
    "true_personality_labels = true_personality_labels.append({'Character': 'James Rhodes', 'Personality': 'ISTJ'}, ignore_index=True)\n",
    "true_personality_labels = true_personality_labels.append({'Character': 'Aldrich Killian', 'Personality': 'ENTJ'}, ignore_index=True)\n",
    "true_personality_labels = true_personality_labels.append({'Character': 'Maya Hansen', 'Personality': 'INFJ'}, ignore_index=True)\n",
    "true_personality_labels = true_personality_labels.append({'Character': 'Ellen Brandt', 'Personality': 'ISTP'}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
